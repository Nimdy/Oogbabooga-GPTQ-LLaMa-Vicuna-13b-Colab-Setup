{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "id": "6mM6S89sLp7s",
        "outputId": "ada80de4-7015-4518-dd0b-38ad22dbc3f4"
      },
      "outputs": [],
      "source": [
        "# @title Oogbabooga Colab Notebook Beta Project\n",
        "# @markdown TL;DR: Runtime > Run All (âŒ˜/Ctrl+F9). Takes about 5 minutes to start. You will be promped to authorize Google Drive access.\n",
        "# @markdown To prevent Colab from disconnecting you and give you more time to work, you can run the Colab on low GPU mode.\n",
        "# @markdown Shoutout to this... I will add their name later. I can't remember.\n",
        "%%html\n",
        "<audio src=\"https://github.com/anars/blank-audio/raw/master/1-hour-of-silence.mp3\" autoplay muted loop controls />"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "n9l3yw0NLp7t"
      },
      "outputs": [],
      "source": [
        "# @title Initialization\n",
        "# @markdown In this cell, all the necessary variables that will be used throughout the notebook are initialized. \n",
        "# @markdown This includes the Google Drive folder where the data is stored, the path where Google Drive will be mounted, the URL of the text-generation repository, and others.\n",
        "# @markdown Always read the source code of the notebook before running it because it may contain malicious code and some bad person may have changed it and can steal your data.\n",
        "\n",
        "# -*- coding: utf-8 -*-\n",
        "import os\n",
        "import subprocess\n",
        "from pathlib import Path\n",
        "from google.colab import drive\n",
        "from socket import gethostname, gethostbyname\n",
        "from requests import get\n",
        "from psutil import virtual_memory\n",
        "\n",
        "# Variables\n",
        "google_drive_folder = \"Colab Data/Oogbabooga\"\n",
        "google_drive_mount_path = \"/content/drive\"\n",
        "google_drive_data_directory_relative_path = google_drive_folder\n",
        "google_drive_data_directory_path = f\"{google_drive_mount_path}/MyDrive/{google_drive_data_directory_relative_path}\"\n",
        "text_generation_repo_url = 'https://github.com/oobabooga/text-generation-webui.git'\n",
        "gptq_repo_url = 'https://github.com/oobabooga/GPTQ-for-LLaMa.git -b cuda'\n",
        "model_name = 'anon8231489123/vicuna-13b-GPTQ-4bit-128g'\n",
        "model_file_url = f'https://huggingface.co/{model_name}/resolve/main/vicuna-13b-4bit-128g.safetensors'\n",
        "model_file_path = f'{google_drive_data_directory_path}/text-generation-webui/models/{model_name}/vicuna-13b-4bit-128g.safetensors'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NzxxH8xsLp7t",
        "outputId": "3a302445-23ce-4576-e40b-e0ea9a1f08b4"
      },
      "outputs": [],
      "source": [
        "# @title Google Drive Mounting\n",
        "# @markdown In this cell, Google Drive is mounted to the specified path. \n",
        "# @markdown A symbolic link is created between the Google Drive data directory and the current directory for easy access. \n",
        "# @markdown Also, a text file is created in the data directory indicating the notebook that is currently using it.\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount(google_drive_mount_path, force_remount=True)\n",
        "\n",
        "google_drive_data_directory_relative_path = google_drive_folder\n",
        "google_drive_data_directory_path = f\"{google_drive_mount_path}/MyDrive/{google_drive_data_directory_relative_path}\"\n",
        "!mkdir -p \"{google_drive_data_directory_path}\"\n",
        "!ln -nsf \"{google_drive_data_directory_path}\" ./data\n",
        "!touch \"data/This folder is used by the Colab notebook for the Oogbabooga files and models.txt\"\n",
        "print(f\"Data will be stored in Google Drive folder: \\\"{google_drive_data_directory_relative_path}\\\", which is mounted under \\\"{google_drive_data_directory_path}\\\"\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YMYpdU9ALp7t",
        "outputId": "98d7cbf9-2551-49ef-c87a-8c355e5b543c"
      },
      "outputs": [],
      "source": [
        "# @title GPU Information\n",
        "# @markdown This cell checks if the current Colab instance is connected to a GPU. \n",
        "# @markdown It uses the `nvidia-smi` command to fetch GPU info and prints it.\n",
        "\n",
        "# GPU Info\n",
        "gpu_info = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, text=True).stdout\n",
        "if 'failed' in gpu_info:\n",
        "    print('Not connected to a GPU')\n",
        "else:\n",
        "    print(gpu_info)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "19IcdW7wLp7t",
        "outputId": "aafbee4e-2509-40ef-f512-15a45b83ce44"
      },
      "outputs": [],
      "source": [
        "# @title RAM Information\n",
        "# @markdown This cell prints the total available RAM on the current Colab instance. \n",
        "# @markdown It also checks if the instance is a high-RAM runtime by comparing the total RAM with a threshold value.\n",
        "\n",
        "# RAM Info\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "if ram_gb < 20:\n",
        "    print('Not using a high-RAM runtime')\n",
        "else:\n",
        "    print('You are using a high-RAM runtime!')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "gZda6SjdLp7u",
        "outputId": "737805a3-d0cf-4753-99d9-c45604f967ea"
      },
      "outputs": [],
      "source": [
        "# @title Text Generation WebUI Setup\n",
        "# @markdown Here, the 'text-generation-webui' repository is cloned from GitHub into the data directory on Google Drive if it does not exist already. \n",
        "# @markdown Then, the requirements for the repository are installed using pip.\n",
        "\n",
        "# Text Generation WebUI Setup\n",
        "if not os.path.exists(f\"{google_drive_data_directory_path}/text-generation-webui\"):\n",
        "    for _ in range(5):  # Try up to 5 times, sometimes the TLS transport fails\n",
        "        try:\n",
        "            !git clone {text_generation_repo_url} \"{google_drive_data_directory_path}/text-generation-webui\" && break  # If successful, break the loop\n",
        "        except Exception:\n",
        "            pass  # If not successful, pass and retry\n",
        "else:\n",
        "    print('text-generation-webui already exists. Skipping clone operation.')\n",
        "\n",
        "os.chdir(f\"{google_drive_data_directory_path}/text-generation-webui\")\n",
        "!pip install -r requirements.txt\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PI6XzkZGLp7u",
        "outputId": "9bb401d3-202c-46cc-b50a-550107e988b2"
      },
      "outputs": [],
      "source": [
        "# @title 4-bit Mode Support Setup\n",
        "# @markdown In this cell, the 'GPTQ-for-LLaMa' repository is cloned from GitHub into the repositories directory under 'text-generation-webui' if it does not exist already. \n",
        "# @markdown The necessary requirements for this repository are installed and the setup script is run.\n",
        "\n",
        "# Clone GPTQ repository and setup\n",
        "if not os.path.exists('repositories/GPTQ-for-LLaMa'):\n",
        "    !mkdir -p repositories\n",
        "    os.chdir('repositories')\n",
        "    for _ in range(5):  # Try up to 5 times, sometimes the TLS transport fails\n",
        "        try:\n",
        "            !git clone -b cuda https://github.com/oobabooga/GPTQ-for-LLaMa.git && break  # If successful, break the loop\n",
        "        except Exception:\n",
        "            pass  # If not successful, pass and retry\n",
        "    os.chdir('GPTQ-for-LLaMa')\n",
        "    !pip install ninja\n",
        "    !pip install -r requirements.txt\n",
        "    !python setup_cuda.py install\n",
        "else:\n",
        "    print('GPTQ-for-LLaMa repository already exists. Skipping setup.')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KyR856GALp7u",
        "outputId": "2670b271-4ea0-4673-81f5-e196ff49ad55"
      },
      "outputs": [],
      "source": [
        "# @title Download Model\n",
        "# @markdown The model is downloaded from Hugging Face and stored in the text-generation-webui directory. \n",
        "# @markdown The model is downloaded only if it does not exist already.\n",
        "\n",
        "# Download Model\n",
        "os.chdir(f\"{google_drive_data_directory_path}/text-generation-webui\")\n",
        "model_path = \"models/anon8231489123_vicuna-13b-GPTQ-4bit-128g/vicuna-13b-4bit-128g.safetensors\"\n",
        "if not os.path.isfile(model_path):\n",
        "    !python download-model.py --text-only anon8231489123/vicuna-13b-GPTQ-4bit-128g\n",
        "    !wget https://huggingface.co/anon8231489123/vicuna-13b-GPTQ-4bit-128g/resolve/main/vicuna-13b-4bit-128g.safetensors\n",
        "    !ls -lisa\n",
        "    # Move Model File\n",
        "    !mv vicuna-13b-4bit-128g.safetensors {model_path}\n",
        "else:\n",
        "    print(f\"Model file {model_path} already exists. Skipping download.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 462
        },
        "id": "kUsnVzl4Lp7u",
        "outputId": "2c563de3-bb6e-48c0-e8f9-dd8af952b807"
      },
      "outputs": [],
      "source": [
        "# @title Run Application\n",
        "# @markdown In the final cell, the necessary version of Pillow is installed, and the application is run with the specified model and parameters.\n",
        "\n",
        "# Run Application\n",
        "!pip install --ignore-installed Pillow==9.3.0\n",
        "!python server.py --share --model anon8231489123_vicuna-13b-GPTQ-4bit-128g --model_type llama --chat --wbits 4 --groupsize 128\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
